{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10888709,"sourceType":"datasetVersion","datasetId":6766254}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:36.909760Z","iopub.execute_input":"2025-04-05T19:28:36.910061Z","iopub.status.idle":"2025-04-05T19:28:38.064077Z","shell.execute_reply.started":"2025-04-05T19:28:36.910030Z","shell.execute_reply":"2025-04-05T19:28:38.060181Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/uzbek-language-dataset-sentence-and-audio/dataset_dict.json\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00001-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/state.json\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00006-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/dataset_info.json\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00007-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00008-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00005-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00002-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00003-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00004-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00009-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/test/data-00000-of-00010.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00018-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00066-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00047-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00080-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00002-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00015-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00056-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00040-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00068-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00043-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00006-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/state.json\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/dataset_info.json\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00079-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00055-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00004-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00072-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00060-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00074-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00009-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00042-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00030-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00028-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00037-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00032-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00035-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00020-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00071-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00069-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00077-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00000-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00073-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00059-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00003-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00049-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00063-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00038-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00067-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00012-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00075-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00052-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00044-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00005-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00014-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00021-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00026-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00013-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00019-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00054-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00041-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00033-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00057-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00008-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00065-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00022-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00036-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00058-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00050-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00011-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00053-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00051-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00062-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00025-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00064-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00061-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00045-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00048-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00081-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00023-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00007-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00034-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00076-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00001-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00016-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00078-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00010-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00070-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00017-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00031-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00027-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00046-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00039-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00029-of-00082.arrow\n/kaggle/input/uzbek-language-dataset-sentence-and-audio/train/data-00024-of-00082.arrow\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_from_disk\n\ncommon_voice_train = load_from_disk(\"/kaggle/input/uzbek-language-dataset-sentence-and-audio/train\")\ncommon_voice_test = load_from_disk(\"/kaggle/input/uzbek-language-dataset-sentence-and-audio/test\")\nprint(common_voice_train)\nprint(common_voice_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:38.064979Z","iopub.execute_input":"2025-04-05T19:28:38.065433Z","iopub.status.idle":"2025-04-05T19:28:40.016463Z","shell.execute_reply.started":"2025-04-05T19:28:38.065383Z","shell.execute_reply":"2025-04-05T19:28:40.015548Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading dataset from disk:   0%|          | 0/82 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b5bf2a4304d472a8d9b2bd4a87fa06a"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['input_features', 'labels'],\n    num_rows: 42242\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 4928\n})\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"sample = common_voice_train[65]\n\n# Input features\ninput_feats = sample[\"input_features\"]\nprint(\"Input shape:\", len(input_feats), \"x\", len(input_feats[0]))  # (time_steps x 80)\n\n# Label\nprint(\"Label:\", sample[\"labels\"])\nprint(\"Label length:\", len(sample[\"labels\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:40.017385Z","iopub.execute_input":"2025-04-05T19:28:40.017760Z","iopub.status.idle":"2025-04-05T19:28:40.155746Z","shell.execute_reply.started":"2025-04-05T19:28:40.017737Z","shell.execute_reply":"2025-04-05T19:28:40.154703Z"}},"outputs":[{"name":"stdout","text":"Input shape: 80 x 3000\nLabel: [50258, 50337, 50359, 50363, 8524, 73, 345, 20291, 4116, 75, 1083, 72, 10806, 4097, 71, 9229, 72, 277, 2282, 270, 559, 27442, 7852, 2604, 89, 742, 14697, 5843, 50257]\nLabel length: 29\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"sample = common_voice_test[66]\n\n# Input features\ninput_feats = sample[\"input_features\"]\nprint(\"Input shape:\", len(input_feats), \"x\", len(input_feats[0]))  # (time_steps x 80)\n\n# Label\nprint(\"Label:\", sample[\"labels\"])\nprint(\"Label length:\", len(sample[\"labels\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:40.158086Z","iopub.execute_input":"2025-04-05T19:28:40.158323Z","iopub.status.idle":"2025-04-05T19:28:40.282266Z","shell.execute_reply.started":"2025-04-05T19:28:40.158306Z","shell.execute_reply":"2025-04-05T19:28:40.281398Z"}},"outputs":[{"name":"stdout","text":"Input shape: 80 x 3000\nLabel: [50258, 50337, 50359, 50363, 4575, 3015, 4842, 8206, 10072, 1299, 1371, 514, 50257]\nLabel length: 13\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate bitsandbytes peft\n!pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118  # Install GPU-supported PyTorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:40.283939Z","iopub.execute_input":"2025-04-05T19:28:40.284233Z","iopub.status.idle":"2025-04-05T19:28:51.841089Z","shell.execute_reply.started":"2025-04-05T19:28:40.284210Z","shell.execute_reply":"2025-04-05T19:28:51.839897Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate, bitsandbytes\nSuccessfully installed bitsandbytes-0.45.4 evaluate-0.4.3\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:51.842343Z","iopub.execute_input":"2025-04-05T19:28:51.842684Z","iopub.status.idle":"2025-04-05T19:28:55.805374Z","shell.execute_reply.started":"2025-04-05T19:28:51.842653Z","shell.execute_reply":"2025-04-05T19:28:55.803921Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model_name_or_path = \"openai/whisper-small\"\nlanguage = \"Uzbek\"\nlanguage_abbr = \"uz\"\ntask = \"transcribe\"\ndataset_name = \"mozilla-foundation/common_voice_13_0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:55.806579Z","iopub.execute_input":"2025-04-05T19:28:55.806932Z","iopub.status.idle":"2025-04-05T19:28:55.811538Z","shell.execute_reply.started":"2025-04-05T19:28:55.806897Z","shell.execute_reply":"2025-04-05T19:28:55.810354Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:55.812509Z","iopub.execute_input":"2025-04-05T19:28:55.812848Z","iopub.status.idle":"2025-04-05T19:28:57.264048Z","shell.execute_reply.started":"2025-04-05T19:28:55.812808Z","shell.execute_reply":"2025-04-05T19:28:57.263193Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0f6763b2ca4f98954a7275cbe7c45a"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import WhisperTokenizer\n\ntokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:57.264778Z","iopub.execute_input":"2025-04-05T19:28:57.265235Z","iopub.status.idle":"2025-04-05T19:28:58.777222Z","shell.execute_reply.started":"2025-04-05T19:28:57.265210Z","shell.execute_reply":"2025-04-05T19:28:58.776217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647ac08163b949ba8e169fd8c675e8cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb0cc7887f545e79dc264e91ed2fc8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52df1e83535b49b09ea3a5b2e37827df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85272e9e356e4f63b0042737b8696bec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9dec98703bc4cf199d3f7c572d48a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302350916f0043da87aadfe4651dc9e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f54db689d46341a2b7d31442d6e2bd6f"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:28:58.778069Z","iopub.execute_input":"2025-04-05T19:28:58.778361Z","iopub.status.idle":"2025-04-05T19:29:02.289617Z","shell.execute_reply.started":"2025-04-05T19:28:58.778338Z","shell.execute_reply":"2025-04-05T19:29:02.288865Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport torch\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Pad input features\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # Convert labels to tensors and pad them\n        labels = [torch.tensor(feature[\"labels\"], dtype=torch.long) for feature in features]\n        labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=self.processor.tokenizer.pad_token_id)\n\n        # Replace padding tokens (50257) with -100 to ignore them in loss calculation\n        labels_padded = labels_padded.masked_fill(labels_padded == self.processor.tokenizer.pad_token_id, -100)\n\n        # If all labels start with BOS token, remove it\n        bos_token_id = self.processor.tokenizer.bos_token_id\n        if bos_token_id is not None and (labels_padded[:, 0] == bos_token_id).all().item():\n            labels_padded = labels_padded[:, 1:]\n\n        batch[\"labels\"] = labels_padded\n        return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:02.290514Z","iopub.execute_input":"2025-04-05T19:29:02.291049Z","iopub.status.idle":"2025-04-05T19:29:02.298311Z","shell.execute_reply.started":"2025-04-05T19:29:02.291018Z","shell.execute_reply":"2025-04-05T19:29:02.297394Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    #max_label_length=448  # Can be adjusted\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:02.299384Z","iopub.execute_input":"2025-04-05T19:29:02.299722Z","iopub.status.idle":"2025-04-05T19:29:02.317310Z","shell.execute_reply.started":"2025-04-05T19:29:02.299693Z","shell.execute_reply":"2025-04-05T19:29:02.316590Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip install -U bitsandbytes\n!pip install accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:02.320493Z","iopub.execute_input":"2025-04-05T19:29:02.320708Z","iopub.status.idle":"2025-04-05T19:29:09.729252Z","shell.execute_reply.started":"2025-04-05T19:29:02.320689Z","shell.execute_reply":"2025-04-05T19:29:09.727882Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.4)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\nimport torch\n\nif torch.cuda.is_available():\n\tmodel = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")\nelse:\n\tmodel = WhisperForConditionalGeneration.from_pretrained(model_name_or_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:09.731659Z","iopub.execute_input":"2025-04-05T19:29:09.732173Z","iopub.status.idle":"2025-04-05T19:29:31.275101Z","shell.execute_reply.started":"2025-04-05T19:29:09.732107Z","shell.execute_reply":"2025-04-05T19:29:31.274070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17308ae27c1049a68b016336fb400fe4"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79fe23e648b74058840bcfa3d3f6c829"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"810a9e15665b49a090aa214ae931590d"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\nfrom peft import LoraConfig, get_peft_model, PeftModel, LoraModel\n\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:31.276199Z","iopub.execute_input":"2025-04-05T19:29:31.276969Z","iopub.status.idle":"2025-04-05T19:29:31.847322Z","shell.execute_reply.started":"2025-04-05T19:29:31.276848Z","shell.execute_reply":"2025-04-05T19:29:31.846533Z"}},"outputs":[{"name":"stdout","text":"trainable params: 3,538,944 || all params: 245,273,856 || trainable%: 1.4429\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:31.848055Z","iopub.execute_input":"2025-04-05T19:29:31.848276Z","iopub.status.idle":"2025-04-05T19:29:31.851828Z","shell.execute_reply.started":"2025-04-05T19:29:31.848257Z","shell.execute_reply":"2025-04-05T19:29:31.851062Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import os\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import WhisperForConditionalGeneration\n\n \n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name_or_path)\nmodel = model.to(\"cuda\")  # Move model to GPU\n\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = torch.nn.DataParallel(model) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:31.852602Z","iopub.execute_input":"2025-04-05T19:29:31.852912Z","iopub.status.idle":"2025-04-05T19:29:33.908774Z","shell.execute_reply.started":"2025-04-05T19:29:31.852880Z","shell.execute_reply":"2025-04-05T19:29:33.907789Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"common_voice_train = common_voice_train.select(range(42000))\ncommon_voice_test = common_voice_test.select(range(4200))\ncommon_voice_train, common_voice_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:33.909763Z","iopub.execute_input":"2025-04-05T19:29:33.910040Z","iopub.status.idle":"2025-04-05T19:29:33.932140Z","shell.execute_reply.started":"2025-04-05T19:29:33.910019Z","shell.execute_reply":"2025-04-05T19:29:33.931302Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['input_features', 'labels'],\n     num_rows: 42000\n }),\n Dataset({\n     features: ['input_features', 'labels'],\n     num_rows: 4200\n }))"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import os\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import WhisperForConditionalGeneration\n\n# ✅ Define training arguments\nfrom transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-small-finetuned\",  # Directory to save checkpoints\n    per_device_train_batch_size=4,  # Small batch size for training\n    per_device_eval_batch_size=4,  # Smaller batch size for evaluation\n    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n    learning_rate=1e-4,  # Learning rate\n    warmup_steps=50,  # Warmup steps for learning rate scheduler\n    num_train_epochs=2,  # Number of training epochs\n    save_strategy=\"epoch\",  # Save at the end of each epoch\n    save_total_limit=2,  # Keep only the last 2 checkpoints\n    evaluation_strategy=\"steps\",  # Evaluate every N steps\n    eval_steps=500,\n    logging_steps=100,  # Log training metrics every 100 steps\n    generation_max_length=128,  # Maximum length for generation during evaluation\n    fp16=True,  # Use mixed precision to reduce memory usage\n    remove_unused_columns=False,  # Remove unused columns from the dataset to save memory\n    optim=\"adamw_bnb_8bit\",  # Use 8-bit AdamW optimizer to reduce memory usage\n    report_to=\"none\", # Disable reporting to external services (e.g., Weights & Biases)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:33.933017Z","iopub.execute_input":"2025-04-05T19:29:33.933299Z","iopub.status.idle":"2025-04-05T19:29:33.972342Z","shell.execute_reply.started":"2025-04-05T19:29:33.933253Z","shell.execute_reply":"2025-04-05T19:29:33.971217Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n# ✅ Create trainer\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=common_voice_train,  # Your dataset\n    eval_dataset=common_voice_test,\n    data_collator=data_collator,\n    tokenizer=processor.feature_extractor\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:33.973455Z","iopub.execute_input":"2025-04-05T19:29:33.973792Z","iopub.status.idle":"2025-04-05T19:29:33.995139Z","shell.execute_reply.started":"2025-04-05T19:29:33.973758Z","shell.execute_reply":"2025-04-05T19:29:33.994220Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-20-0808186943d6>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:15:29.813719Z","iopub.execute_input":"2025-04-06T05:15:29.814071Z","iopub.status.idle":"2025-04-06T05:15:30.169539Z","shell.execute_reply.started":"2025-04-06T05:15:29.814046Z","shell.execute_reply":"2025-04-06T05:15:30.168356Z"}},"outputs":[{"name":"stdout","text":"Sun Apr  6 05:15:29 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   72C    P0             29W /   70W |    9197MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   71C    P0             30W /   70W |    5965MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"common_voice_train, common_voice_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:34.255705Z","iopub.execute_input":"2025-04-05T19:29:34.255969Z","iopub.status.idle":"2025-04-05T19:29:34.261702Z","shell.execute_reply.started":"2025-04-05T19:29:34.255946Z","shell.execute_reply":"2025-04-05T19:29:34.260914Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['input_features', 'labels'],\n     num_rows: 42000\n }),\n Dataset({\n     features: ['input_features', 'labels'],\n     num_rows: 4200\n }))"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"trainer.train()  # Start training from scratch\n\n# Define the directory to save the final model\nfinal_model_dir = \"./whisper-small-finetuned\"\n\n# Save the fine-tuned model and processor\ntrainer.save_model(final_model_dir)\nprocessor.save_pretrained(final_model_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:29:34.262506Z","iopub.execute_input":"2025-04-05T19:29:34.262795Z","iopub.status.idle":"2025-04-06T05:14:54.727124Z","shell.execute_reply.started":"2025-04-05T19:29:34.262758Z","shell.execute_reply":"2025-04-06T05:14:54.688844Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2624' max='2624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2624/2624 9:45:02, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.588400</td>\n      <td>0.596903</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.428100</td>\n      <td>0.480948</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.196700</td>\n      <td>0.416555</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.170100</td>\n      <td>0.374123</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.145300</td>\n      <td>0.335988</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"### Pushing to Hugging Face repository","metadata":{}},{"cell_type":"code","source":"!pip install -U huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:49:05.070566Z","iopub.execute_input":"2025-04-06T05:49:05.070957Z","iopub.status.idle":"2025-04-06T05:49:12.035644Z","shell.execute_reply.started":"2025-04-06T05:49:05.070928Z","shell.execute_reply":"2025-04-06T05:49:12.034706Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\nDownloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.2/481.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.29.0\n    Uninstalling huggingface-hub-0.29.0:\n      Successfully uninstalled huggingface-hub-0.29.0\nSuccessfully installed huggingface_hub-0.30.1\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your token when prompted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:31:55.458739Z","iopub.execute_input":"2025-04-06T05:31:55.459164Z","iopub.status.idle":"2025-04-06T05:31:55.490807Z","shell.execute_reply.started":"2025-04-06T05:31:55.459124Z","shell.execute_reply":"2025-04-06T05:31:55.489952Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66296223ed874835addbf7d5f3120eae"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder\n\n# 🔐 Paste your write token here (keep it secret!)\nhf_token = \".........................\"\n\n# Save the token to use with huggingface_hub\nHfFolder.save_token(hf_token)\n\n# Test if login works\napi = HfApi()\nprint(\"Logged in as:\", api.whoami()[\"name\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:53:03.642112Z","iopub.execute_input":"2025-04-06T05:53:03.642466Z","iopub.status.idle":"2025-04-06T05:53:03.764802Z","shell.execute_reply.started":"2025-04-06T05:53:03.642440Z","shell.execute_reply":"2025-04-06T05:53:03.763935Z"}},"outputs":[{"name":"stdout","text":"Logged in as: javokhirraimov\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from huggingface_hub import create_repo, upload_folder\n\nrepo_id = \"javokhirraimov/uzbek-stt-whisper-small\"\nlocal_path = \"/kaggle/working/whisper-small-finetuned\"\n\n# Create and upload\ncreate_repo(repo_id=repo_id, exist_ok=True)\nupload_folder(\n    repo_id=repo_id,\n    folder_path=local_path,\n    commit_message=\"Upload fine-tuned Whisper Small model for Uzbek STT\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:54:56.579427Z","iopub.execute_input":"2025-04-06T05:54:56.579812Z","iopub.status.idle":"2025-04-06T05:55:08.940369Z","shell.execute_reply.started":"2025-04-06T05:54:56.579780Z","shell.execute_reply":"2025-04-06T05:55:08.939419Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/javokhirraimov/uzbek-stt-whisper-small/commit/f884cb27635db55f8a21d9e2e988acce01a1d016', commit_message='Upload fine-tuned Whisper Small model for Uzbek STT', commit_description='', oid='f884cb27635db55f8a21d9e2e988acce01a1d016', pr_url=None, repo_url=RepoUrl('https://huggingface.co/javokhirraimov/uzbek-stt-whisper-small', endpoint='https://huggingface.co', repo_type='model', repo_id='javokhirraimov/uzbek-stt-whisper-small'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"### Test the model","metadata":{}},{"cell_type":"code","source":"# prompt: lets test my model with examlpe from my test data\n# Get a sample from the test dataset\nfirst_sample = common_voice_test[1200]\n\n# Prepare the input for the model\ninput_features = torch.tensor(first_sample[\"input_features\"]).unsqueeze(0).to(\"cuda\")\n\n# Generate prediction using the model\npredicted_ids = model.generate(input_features)\n\n# Decode the prediction\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n\n# Get the actual label\nlabel = processor.decode(first_sample[\"labels\"], skip_special_tokens=True)\n\nprint(\"Predicted transcription:\", transcription)\nprint(\"Actual transcription:\", label)\n\n# Compute WER or other metrics (optional)\n# wer = metric.compute(predictions=[transcription], references=[label])\n# print(\"WER:\", wer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:16:32.127756Z","iopub.execute_input":"2025-04-06T05:16:32.128161Z","iopub.status.idle":"2025-04-06T05:16:38.939369Z","shell.execute_reply.started":"2025-04-06T05:16:32.128134Z","shell.execute_reply":"2025-04-06T05:16:38.938416Z"}},"outputs":[{"name":"stderr","text":"Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Predicted transcription: Kozga aytaman qari quloqqa eshit ogizga gapirma                                                                                                                                                                                                                                                                                                                                                                                                                                       \nActual transcription:  Kozga aytaman Qara quloqqa Eshit ogizga Gapirma\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"first_sample = common_voice_test[1000]\n\n# Preprocessing for model\ninput_features = torch.tensor(first_sample[\"input_features\"]).unsqueeze(0).to(\"cuda\")\n\n# Model generations config\npredicted_ids = model.generate(\n    input_features,\n    max_length=100,\n    repetition_penalty=2.0,\n    no_repeat_ngram_size=3,\n    length_penalty=0.8,\n    num_beams=5,\n    early_stopping=True\n)\n\n# decoding  transcription \ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n\n# Haqiqiy transkripsiyani olish\nlabel = processor.decode(first_sample[\"labels\"], skip_special_tokens=True)\n\nprint(\"Predicted transcription:\", transcription)\nprint(\"Actual transcription:\", label)\n\n# WER yoki boshqa metrikalarni hisoblash (ixtiyoriy)\n# wer = metric.compute(predictions=[transcription], references=[label])\n# print(\"WER:\", wer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:18:46.675798Z","iopub.execute_input":"2025-04-06T05:18:46.676256Z","iopub.status.idle":"2025-04-06T05:18:49.349505Z","shell.execute_reply.started":"2025-04-06T05:18:46.676225Z","shell.execute_reply":"2025-04-06T05:18:49.348490Z"}},"outputs":[{"name":"stdout","text":"Predicted transcription: Judayam qattiqqavozdan taraxt bolib qolgan edi  deb oylaydi tajribatida shunday emas qildi qovozidan taraqd bolib kongan edi ijda yordamlashadi tarixga boladigan edi aylab qoyilmagan edi uchta yetkazilmasliklari bnida\nActual transcription: judayam qattiq ovozdan karaxt bolib qolgan edi\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"first_sample = common_voice_test[117]\n\n# Preprocessing for model\ninput_features = torch.tensor(first_sample[\"input_features\"]).unsqueeze(0).to(\"cuda\")\n\n# Model generations config\npredicted_ids = model.generate(\n    input_features,\n    max_length=100,\n    repetition_penalty=2.0,\n    no_repeat_ngram_size=3,\n    length_penalty=0.8,\n    num_beams=5,\n    early_stopping=True\n)\n\n# decoding  transcription \ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n\n# Haqiqiy transkripsiyani olish\nlabel = processor.decode(first_sample[\"labels\"], skip_special_tokens=True)\n\nprint(\"Predicted transcription:\", transcription)\nprint(\"Actual transcription:\", label)\n\n# WER yoki boshqa metrikalarni hisoblash (ixtiyoriy)\n# wer = metric.compute(predictions=[transcription], references=[label])\n# print(\"WER:\", wer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T06:37:09.507650Z","iopub.execute_input":"2025-04-06T06:37:09.508031Z","iopub.status.idle":"2025-04-06T06:37:12.286661Z","shell.execute_reply.started":"2025-04-06T06:37:09.508003Z","shell.execute_reply":"2025-04-06T06:37:12.285962Z"}},"outputs":[{"name":"stdout","text":"Predicted transcription: Samarqand shahri kochalardan ulkan bayroq golib otildi  deb oshirilgan ham mumtozalarni bayrog qolib otirdi Oylayotgani olamizda bolayotgachalaridan ulkanbayroq olib yotildi Otildi Oylangizmi bilan kuchalardan uygan bayrogga olib otilib otilding\nActual transcription: Samarqand shahri kochalardan ulkan bayroq olib otildi\n","output_type":"stream"}],"execution_count":58}]}